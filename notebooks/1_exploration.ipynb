{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "Dans ce notebook, on va faire une premi√®re exploration des donn√©es. On va compter le nombre de phrase et de tokens, tracer la distribution des longueurs des phrases, analyser les cat√©gories du discours et leurs directions syntaxiques pr√©viligi√©es. Nous allons aussi voir quelles sont les collocations statistiques de la langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TREEBANK = \"../data/sud_naija-NSC.with_prediction.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le fichier est t-il bien pr√©sent ?\n",
    "try:\n",
    "    with open(PATH_TREEBANK, \"r\") as f:\n",
    "        print(\"Le fichier est pr√©sent au chemin : {}\".format(PATH_TREEBANK))\n",
    "        pass\n",
    "except FileNotFoundError:\n",
    "    print(\"Le fichier n'est PAS , revoyez le chemin : {}\".format(PATH_TREEBANK))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut commencer √† parser le fichier avec la librairie conllup\n",
    "from conllup.conllup import readConlluFile \n",
    "sentences = readConlluFile(PATH_TREEBANK)\n",
    "\n",
    "if len(sentences) == 0:\n",
    "    raise ValueError(\"Le fichier est vide !\")\n",
    "else :\n",
    "    print(\"Le fichier contient {} phrases\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quoi ressemble notre objet \"sentence\"\n",
    "import json\n",
    "print(sentences[0]['metaJson'][\"text\"])\n",
    "print(json.dumps(sentences[0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons retrouver, dans `sentence[\"metaJson\"]`, les meta-informations des phrases du conllu :\n",
    "- `sent_id`: identifiant de la phrase dans le treebank.\n",
    "- `sound_url`: lien url de l'audio de la phrase (ce treebank NaijaSynCor est un corpus oral) .\n",
    "- `speaker_id`: identifiant (anonymis√©) du locuteur.\n",
    "- `text`: version \"prosodique\" du texte. Les ponctuations sont plus riches et des marqueurs prosodiques (retranscrivant l'oral) sont ajout√©s.\n",
    "- `text_en`: traduction anglaise.\n",
    "- `text_ortho`: version litt√©raire du texte.\n",
    "- `timestamp`: date de la derni√®re annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voici donc ces meta-donn√©es seulement\n",
    "print(json.dumps(sentences[0]['metaJson'], indent=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment acc√©der aux tokens ?\n",
    "# ils sont imbriqu√©s dans sentence['treeJson']['nodesJson']\n",
    "sentence_tokens = sentences[0]['treeJson']['nodesJson']\n",
    "print(json.dumps(sentence_tokens, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les tokens sont donc dans un dictionnaire avec comme cl√© l'ID du token \n",
    "# et comme valeur un dictionnaire avec les informations du token\n",
    "\n",
    "# Regardons le premier token\n",
    "print(json.dumps(sentence_tokens['1'], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici la composition d'un token au format json (voir [sp√©cifications officielles ici](https://universaldependencies.org/format.html) )\n",
    "- `ID`: Identifiant du token dans la phrase,\n",
    "- `FORM`: Forme du token,\n",
    "- `LEMMA`: Lemme du token,\n",
    "- `UPOS`: Cat√©gorie du discours universelle (Universal Part Of Speech)\n",
    "- `XPOS`: Cat√©gorie du discours **optionnelle**\n",
    "- `FEATS`: Liste de features morphologiques de l'inventaire universel ou d'une extension sp√©cifique √† la langue\n",
    "- `HEAD`: Identifiant du gouverneur du token (0 si le token est la t√™te de la phrase)\n",
    "- `DEPREL`: Relation de d√©pendance avec le gouverneur (`root` si le token est la t√™te de la phrase)\n",
    "- `DEPS`: Graphes de d√©pendances am√©lior√©s sous la forme d'une liste de paires t√™te-d√©prel \n",
    "- `MISC`: Toutes les autres donn√©es non universelles peuvent √™tre ins√©r√©es dans cet objet MISC. Voici quelques exemples de valeur misc\n",
    "    - `AlignBegin`: alignement temporelle **du d√©but** du token dans l'audio de la phrase (en milliseconde)\n",
    "    - `AlignEnd`: alignement temporelle de **la fin** du token dans l'audio de la phrase (en milliseconde),\n",
    "    - `Gloss`: glose anglaise\n",
    "    - `upos_pred`: Cat√©gorie du dicours **pr√©dit**\n",
    "    - `head_pred`: Identifiant **pr√©dit** du gouverneur,\n",
    "    - `deprel_pred`: Relation de d√©pendance **pr√©dite**,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöß TODO\n",
    "1) üöß Combien de tokens sont dans le corpus ? \n",
    "\n",
    "PS : Attention, les tokens sont dans `sentence['treeJson']['nodesJson']` sous format dictionnaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "nb_tokens = 0\n",
    "for ....\n",
    "\n",
    "\n",
    "print(\"Le corpus contient {} tokens\".format(nb_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. üöß Quelle est la longueur moyenne d'une phrase dans le corpus ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "longueur_moyenne = ...\n",
    "print(\"Le nombre moyen de tokens par phrase est de {}\".format(longueur_moyenne))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "Regardons nos donn√©es via des graphes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous utilisons matplotlib pour faire faire nos graphiques\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Voici des param√®tres pour que les graphiques soient plus lisibles, changez les √† votre convenance\n",
    "params = {\n",
    "   'axes.labelsize': 8,\n",
    " #  'text.fontsize': 8,\n",
    "   'legend.fontsize': 10,\n",
    "   'xtick.labelsize': 10,\n",
    "   'ytick.labelsize': 10,\n",
    "   'text.usetex': False,\n",
    "   'figure.figsize': [10, 5]\n",
    "   }\n",
    "mpl.rcParams.update(params)\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # pour avoir un fond gris quadrill√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. üöß Affichez la distribution des tokens par longueur de phrase dans un histogramme\n",
    "PS : Vous pouvez ajouter un param√®tre `bins=` √† la fonction `plt.hist()` qui vous permet de choisir le groupement de phrases (`bins=range(1,5)` ne va montrer que la distribution pour les phrases de tailles 1 √† 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Affichons la distribution des tokens par phrase\n",
    "\n",
    "## Recup√©rons le nombre de tokens par phrase dans une liste\n",
    "tokens_per_sentence = []\n",
    "for sentence in sentences:\n",
    "    # ... to add here\n",
    "\n",
    "# ## Tra√ßons l'histogramme\n",
    "plt.hist(tokens_per_sentence, edgecolor='white') \n",
    "plt.title(\"Distribution du nombre de tokens par phrase (ajust√©)\") # ajoute un titre au graphique\n",
    "plt.xlabel(\"Nombre de tokens\") # ajoute un titre √† l'axe des abscisses\n",
    "plt.ylabel(\"Nombre de phrases\") # ajoute un titre √† l'axe des ordonn√©es\n",
    "\n",
    "\n",
    "## Optionel : Ajoutons une ligne verticale rouge pour la moyenne\n",
    "# length_mean = nb_tokens/len(sentences)\n",
    "# plt.axvline(x=length_mean, color='red', linestyle='dashed', linewidth=1) # place la ligne verticale de moyenne (en rouge)\n",
    "# plt.text(length_mean*1.1, plt.ylim()[1]*0.9, 'Moyenne = {:.2f}'.format(length_mean)) # place la l√©gende de la ligne verticale\n",
    "\n",
    "\n",
    "## Affichons le graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faisons une fonction \"counter\" qui, depuis une liste d'√©tiquettes, retourne un dictionnaire avec le nombre d'occurences de chaque √©tiquette\n",
    "# le compteur doit √™tre ordonn√© par ordre alphab√©tique\n",
    "def make_counter(labels):\n",
    "    counter = {}\n",
    "    for label in labels:\n",
    "        if label in counter:\n",
    "            counter[label] += 1\n",
    "        else:\n",
    "            counter[label] = 1\n",
    "    # sort alphabetically before return\n",
    "    counter = {k: v for k, v in sorted(counter.items(), key=lambda item: item[0])}\n",
    "    return counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# testons sur une liste exemple\n",
    "labels = [\"VERB\", \"NOUN\", \"NOUN\", \"VERB\", \"VERB\"]\n",
    "print(make_counter(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. üöß Faire une liste contenant toutes les UPOS du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "upos_list = []\n",
    "# iterer sur les phrases puis les tokens\n",
    "\n",
    "print(upos_list[:5])\n",
    "print(len(upos_list))\n",
    "print(json.dumps(make_counter(upos_list), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tra√ßons le barplot\n",
    "upos_counter = make_counter(upos_list)\n",
    "plt.bar(upos_counter.keys(), upos_counter.values())\n",
    "plt.xticks(rotation=45) # pour que les √©tiquettes soient lisibles (on les tourne de 45¬∞)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les ponctuations sont grandement dominantes, il faudra faire attention √† ce que √ßa ne biaise pas les √©valuations du parseur (il est g√©n√©ralement plus facile de faire des pr√©dictions sur les ponctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance et ordre syntaxique\n",
    "Le naija est t'il une langue √† t√™te initiale ou finale ?\n",
    "\n",
    "5. üöß Calculez le nombre de relation de d√©pendances allant vers la gauche puis vers la droite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "left_n = 0\n",
    "right_n = 0\n",
    "for sentence in sentences:\n",
    "    for token in sentence['treeJson']['nodesJson'].values():\n",
    "        # todo\n",
    "print(\"Il y a {} d√©pendances vers la gauche et {} vers la droite\".format(left_n, right_n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√áa n'a pas l'air de nous dire grand chose. On va analyser la m√™me chose mais par cat√©gorie du discours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour cela, nous allons compter le nombre de d√©pendances allant vers la gauche et vers la droite pour chaque cat√©gorie du discours\n",
    "# Nous allons utiliser un dictionnaire de dictionnaires pour stocker ces informations\n",
    "# Le premier niveau de cl√© sera la cat√©gorie du discours\n",
    "# Le second niveau de cl√© sera la direction de la d√©pendance (left ou right)\n",
    "# La valeur sera le nombre de d√©pendances dans cette cat√©gorie et dans cette direction\n",
    "\n",
    "\n",
    "# ici on l'initialise\n",
    "upos_directions = {}\n",
    "for upos in list(sorted(set(upos_list))):\n",
    "    upos_directions[upos] = {'left': 0, 'right': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. üöß alimentez le dictionnaire `upos_directions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "\n",
    "for sentence in sentences:\n",
    "    for token in sentence['treeJson']['nodesJson'].values():\n",
    "        # todo\n",
    "        \n",
    "upos_directions = {k: v for k, v in sorted(upos_directions.items(), key=lambda item: item[0])}        \n",
    "print(json.dumps(upos_directions, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons maintenant afficher ces informations dans un graphique en barres\n",
    "# Deux options s'offrent √† nous :\n",
    "# - un graphique en barres empil√©es\n",
    "# - un graphique en barres c√¥te √† c√¥te\n",
    "# Nous allons utiliser un graphique en barres c√¥te √† c√¥te\n",
    "\n",
    "import numpy as np\n",
    "# S√©parons les donn√©es dans des listes \"droite\" et \"gauche\"\n",
    "labels = list(upos_directions.keys())\n",
    "left_values = [upos['left'] for upos in upos_directions.values()]\n",
    "right_values = [upos['right'] for upos in upos_directions.values()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35  # √©paisseur des barres\n",
    "index = np.arange(len(labels))  # position des barres\n",
    "\n",
    "# Tra√ßage des barres de gauche\n",
    "left_bars = ax.bar(index, left_values, bar_width, label='Gauche', color='skyblue')\n",
    "\n",
    "# Tra√ßage des barres de gauche\n",
    "right_bars = ax.bar(index + bar_width, right_values, bar_width, label='Droite', color='orange')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('√©tiquettes UPOS')\n",
    "ax.set_ylabel('Comptes')\n",
    "ax.set_title('Distribution des directions des d√©pendances par √©tiquette UPOS : Gauche vs Droite')\n",
    "ax.set_xticks(index + bar_width / 2)  # Position x-axis ticks in the center of the two bars\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Rotate the x-axis labels to 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout to fit everything\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus pas important :  donnez des ex. de DET √† droite du gouverneur \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directions syntaxiques en fonction des cat√©gories du discours GOV et DEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce graphique est int√©ressant, mais on voudrait aussi connaitre la direction en fonction de la \n",
    "# cat√©gorie du discours du gouverneur (HEAD) et afficher cela dans une matrice de position\n",
    "\n",
    "# Pour cela, nous allons d'abord faire notre dictionnaire de dictionnaire de dictionnaire\n",
    "# Le premier niveau de cl√© sera la cat√©gorie du discours du gouverneur (HEAD)\n",
    "# Le second niveau de cl√© sera la cat√©gorie du discours du d√©pendant (DEP)\n",
    "# Le troisi√®me niveau de cl√© sera la direction de la d√©pendance (left ou right)\n",
    "# La valeur sera le nombre de d√©pendances dans cette cat√©gorie et dans cette direction\n",
    "\n",
    "head_dep_directions = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    for token in sentence['treeJson']['nodesJson'].values():\n",
    "        if token['HEAD'] == 0:\n",
    "            # On ne compte pas les cas ou la t√™te est la racine\n",
    "            continue\n",
    "        head_token = sentence['treeJson']['nodesJson'][str(token['HEAD'])]\n",
    "        head_ID = head_token['ID']\n",
    "        head_UPOS = head_token['UPOS']\n",
    "\n",
    "        this_upos = token['UPOS']\n",
    "        if head_UPOS not in head_dep_directions:\n",
    "            head_dep_directions[head_UPOS] = {}\n",
    "\n",
    "        if this_upos not in head_dep_directions[head_UPOS]:\n",
    "            head_dep_directions[head_UPOS][this_upos] = {'left': 0, 'right': 0}\n",
    "\n",
    "        if token['HEAD'] < int(token['ID']):\n",
    "            head_dep_directions[head_UPOS][this_upos]['right'] += 1\n",
    "        else:\n",
    "            head_dep_directions[head_UPOS][this_upos]['left'] += 1\n",
    "\n",
    "# On trie le dictionnaire par ordre alphab√©tique\n",
    "head_dep_directions = {k: v for k, v in sorted(head_dep_directions.items(), key=lambda item: item[0])}\n",
    "# Et on trie les dictionnaires imbriqu√©s par ordre alphab√©tique\n",
    "for head_upos in head_dep_directions:\n",
    "    head_dep_directions[head_upos] = {k: v for k, v in sorted(head_dep_directions[head_upos].items(), key=lambda item: item[0])}\n",
    "\n",
    "\n",
    "print(json.dumps(head_dep_directions, indent=4))\n",
    "\n",
    "# Nous allons maintenant afficher ces informations dans une matrice de position\n",
    "# Pour cela, nous allons utiliser la librairie matplotlib\n",
    "\n",
    "# Nous allons utiliser une fonction de la librairie matplotlib pour afficher la matrice de position\n",
    "# Cette fonction prend en param√®tre une matrice de position et une liste d'√©tiquettes\n",
    "# Elle affiche la matrice de position dans un graphique\n",
    "# Elle retourne un objet \"figure\" qui contient le graphique\n",
    "# Nous allons utiliser cet objet pour ajouter un titre √† notre graphique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour la matricde de position, il nous faut, pour chaque paire de cat√©gories du discours (HEAD, DEP),\n",
    "# un ratio de gouvernance √† droite par rapport au nombre total de gouvernance\n",
    "# Nous allons donc cr√©er un dictionnaire de dictionnaire de dictionnaire de ratio\n",
    "# Le premier niveau de cl√© sera la cat√©gorie du discours du gouverneur (HEAD)\n",
    "# Le second niveau de cl√© sera la cat√©gorie du discours du d√©pendant (DEP)\n",
    "\n",
    "head_dep_ratios = {}\n",
    "head_dep_total = {}\n",
    "\n",
    "for head_upos in head_dep_directions:\n",
    "    head_dep_ratios[head_upos] = {}\n",
    "    head_dep_total[head_upos] = {}\n",
    "    for dep_upos in head_dep_directions[head_upos]:\n",
    "        left_relation = head_dep_directions[head_upos][dep_upos]['left']\n",
    "        right_relation = head_dep_directions[head_upos][dep_upos]['right']\n",
    "        total_relation = left_relation + right_relation\n",
    "        if total_relation < 10:\n",
    "            head_dep_ratios[head_upos][dep_upos] = 0\n",
    "        else:\n",
    "            head_dep_ratios[head_upos][dep_upos] = (right_relation + 0.0001) / total_relation \n",
    "        head_dep_total[head_upos][dep_upos] = total_relation\n",
    "\n",
    "print(json.dumps(head_dep_ratios, indent=4))\n",
    "print(json.dumps(head_dep_total, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons d'abord afficher le nombre de relation par pair de cat√©gories du discours\n",
    "# Il est important de faire cela avant d'afficher le ratio, car le ratio ne veut rien dire si le nombre de relation est trop faible\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Cr√©er un DataFrame pandas pour stocker les donn√©es\n",
    "upos_df = pd.DataFrame(head_dep_total)\n",
    "\n",
    "# Cr√©er la matrice de position avec seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(upos_df, annot=True, cmap=\"Blues\", fmt='d')\n",
    "\n",
    "# Ajouter les titres et labels\n",
    "plt.title('Nombre de d√©pendances par cat√©gorie du discours du gouverneur (HEAD) et du d√©pendant (DEP)')\n",
    "plt.xlabel('Gouverneurs')\n",
    "plt.ylabel('D√©pendants')\n",
    "\n",
    "# Afficher la matrice de position\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS : c'est un peu comme un double clustering sur grew-match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Cr√©er un DataFrame pandas pour stocker les donn√©es\n",
    "upos_df = pd.DataFrame(head_dep_ratios)\n",
    "# Pour les besoins de la visualisation, nous allons sommer les valeurs de 'left' et 'right'\n",
    "# pour chaque paire d'√©tiquettes UPOS\n",
    "\n",
    "# Cr√©er la matrice de position avec seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(upos_df, annot=True, cmap=\"Blues\", fmt='.2f')\n",
    "\n",
    "# Ajouter les titres et labels\n",
    "plt.title('0 -> gauche ; 1->droite')\n",
    "plt.xlabel('Gouverneurs')\n",
    "plt.ylabel('D√©pendants')\n",
    "\n",
    "# Afficher la matrice de position\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions :\n",
    "Dans ce treebank du Naija SUD :\n",
    "- L'auxiliaire est-t'il plus souvent d√©pendant ou gouverneur ? \n",
    "- Une ponctuation peut-elle gouverner une non-ponctuation ? Arriverez vous √† retrouver ces cas dans le treebank ?\n",
    "- Quelles sont les cat√©gories qui gouvernent le plus ?\n",
    "- Et quelles sont les cat√©gories qui gouvernent le moins ?\n",
    "- Quelle est la classe syntaxique qui est la plus contrainte sur son emplacement ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherchons les collocations statistiques\n",
    "> COLLOCATION SIGNIFICATIVE est une collocation habituelle entre deux unit√©s, telles qu'elles se trouvent ensemble plus souvent que leurs fr√©quences respectives et la longueur du texte dans lequel elles apparaissent ne peuvent le pr√©dire \n",
    "\n",
    "Sinclair, 1970, p.150, cit√© et traduit par Williams,2003, p.37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. üöß Calculez les bigrammes et montrez ensuite les plus fr√©quents de la langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = {}\n",
    "for sentence in sentences:\n",
    "    tokens = list(sentence['treeJson']['nodesJson'].values())\n",
    "    # 7. todo\n",
    "\n",
    "bigrams = {k: v for k, v in sorted(bigrams.items(), key=lambda item: item[1], reverse=True)}\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ne peut pas r√©ellement parler de collocation statistique pour le moment. Il faudraut trouver un moyen de normaliser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. üöß Calculez les monogrammes et montrez ensuite les plus fr√©quents de la langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monograms = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = list(sentence['treeJson']['nodesJson'].values())\n",
    "    for indice in range(len(tokens)):\n",
    "        this_token = tokens[indice]\n",
    "\n",
    "        if this_token[\"UPOS\"] == \"PUNCT\":\n",
    "            continue\n",
    "        \n",
    "        this_token_lemma = this_token[\"LEMMA\"]\n",
    "        if this_token_lemma not in monograms:\n",
    "            monograms[this_token_lemma] = 1\n",
    "        else :\n",
    "            monograms[this_token_lemma] += 1\n",
    "monograms = {k: v for k, v in sorted(monograms.items(), key=lambda item: item[1], reverse=True)}  \n",
    "monograms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8bis. üöß Vous pouvez faire un histogramme des tokens les plus fr√©quents, vous y verrez une certaine loi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. üöß Vous pouvez maintenant normaliser la fr√©quences des bigrammes par les fr√©quences des monogrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_normalized = {}\n",
    "\n",
    "for couple in bigrams:\n",
    "    this_freq = monograms[couple[0]]\n",
    "    after_freq = monograms[couple[1]]\n",
    "    if this_freq < 50:\n",
    "        continue\n",
    "    pmi_like = bigrams[couple] / (this_freq * after_freq)\n",
    "    bigrams_normalized[couple] = pmi_like\n",
    "\n",
    "bigrams_normalized = {k: v for k, v in sorted(bigrams_normalized.items(), key=lambda item: item[1], reverse=True)}  \n",
    "bigrams_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quels sont les trigrammes syntaxiques les plus fr√©quents dans ce corpus ?\n",
    "# On d√©finit un trigramme comme un ensemble de trois tokens cons√©cutifs, dont deux sont d√©pendants du troisi√®me\n",
    "\n",
    "trigrams = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = list(sentence['treeJson']['nodesJson'].values())\n",
    "    for i in range(len(tokens) - 2):\n",
    "        token1 = tokens[i]\n",
    "        token2 = tokens[i+1]\n",
    "        token3 = tokens[i+2]\n",
    "\n",
    "        # on ne compte pas si l'un des tokens est une ponctuation\n",
    "        if token1['UPOS'] == 'PUNCT' or token2['UPOS'] == 'PUNCT' or token3['UPOS'] == 'PUNCT':\n",
    "            continue\n",
    "        # if token1['HEAD'] == token2['HEAD'] == token3['HEAD']:\n",
    "        #     # On ne compte pas les trigrammes dont les trois tokens ont le m√™me gouverneur\n",
    "        #     continue\n",
    "        trigram_form = \" , \".join([token1['FORM'], token2['FORM'], token3['FORM']])\n",
    "        if trigram_form not in trigrams:\n",
    "            trigrams[trigram_form] = 0\n",
    "        trigrams[trigram_form] += 1\n",
    "\n",
    "# sort by decreasing frequency\n",
    "trigrams = {k: v for k, v in sorted(trigrams.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(json.dumps(trigrams, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce n'est pas pertinent, on a juste les trigrammes les plus fr√©quents, mais pas\n",
    "# les plus d√©pendants les uns des autres\n",
    "# Il faut donc diviser par la multiplication des fr√©quences de chaque token\n",
    "\n",
    "# On r√©cup√®re donc les fr√©quences des tokens\n",
    "monogrammes = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    for token in sentence['treeJson']['nodesJson'].values():\n",
    "        if token['UPOS'] == 'PUNCT':\n",
    "            continue\n",
    "        if token['FORM'] not in monogrammes:\n",
    "            monogrammes[token['FORM']] = 0\n",
    "        monogrammes[token['FORM']] += 1\n",
    "\n",
    "trigrams_PMI = {}\n",
    "for trigram in trigrams:\n",
    "    token1, token2, token3 = trigram.split(\" , \")\n",
    "    trigrams_PMI[trigram] = trigrams[trigram] / (monogrammes[token1] * monogrammes[token2] * monogrammes[token3])\n",
    "\n",
    "# sort by decreasing frequency\n",
    "trigrams_PMI = {k: v for k, v in sorted(trigrams.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "\n",
    "print(json.dumps(trigrams_PMI, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce n'est encore pas si interessant, puisque nous avons en t√™te des trigrammes qui n'apparaissent qu'une seule fois\n",
    "# Nous allons donc filtrer les trigrammes qui apparaisent moins de N fois (par exemple 10 fois)\n",
    "\n",
    "trigrams_PMI = {}\n",
    "\n",
    "\n",
    "N = 5 # changez cette valeur pour voir les trigrammes qui apparaissent plus ou moins souvent\n",
    "# au plus cette valeur sera basse, au plus on aura de trigrammes irr√©guliers (des \"outliers\" non repr√©sentatifs)\n",
    "# par exemple, en dessous de N=5, vous verrez beaucoup de noms propres et suite de num√©raux\n",
    "\n",
    "for trigram in list(trigrams.keys()):\n",
    "    \n",
    "    if trigrams[trigram] > N:\n",
    "        token1, token2, token3 = trigram.split(\" , \")\n",
    "        trigrams_PMI[trigram] = trigrams[trigram] / (monogrammes[token1] * monogrammes[token2] * monogrammes[token3])\n",
    "\n",
    "trigrams_PMI = {k: v for k, v in sorted(trigrams_PMI.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(json.dumps(trigrams_PMI, indent=4))\n",
    "print(\"{} trigrammes ont √©t√© trouv√©s\".format(len(trigrams_PMI)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "### D'autres concepts interessants\n",
    "- L'espagnol est une langue pro-drop, c'est √† dire que le sujet pronominal d'un verbe peut √™tre effac√©. Qu'en est t-il du naija ? Quel est le ratio de verbe sans sujet ? Est-ce que ce ratio seul suffit √† proposer une hypoth√®se ?\n",
    "- Nous n'avons pas encore tra√ß√© les distributions des longueurs de d√©pendances syntaxique.\n",
    "\n",
    "### Comparaison \n",
    "Vous pouvez trouver dans le dossier `data` d'autres treebanks SUD. On peut donc comparer certains des indicateurs que nous avons calcul√©s ici avec ceux des autres langues.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
