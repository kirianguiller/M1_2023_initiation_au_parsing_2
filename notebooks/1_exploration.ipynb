{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TREEBANK = \"../data/sud_naija-NSC.with_prediction.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le fichier est t-il bien présent ?\n",
    "try:\n",
    "    with open(PATH_TREEBANK, \"r\") as f:\n",
    "        print(\"Le fichier est présent au chemin : {}\".format(PATH_TREEBANK))\n",
    "        pass\n",
    "except FileNotFoundError:\n",
    "    print(\"Le fichier n'est PAS , revoyez le chemin : {}\".format(PATH_TREEBANK))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut commencer à parser le fichier avec la librairie conllup\n",
    "from conllup.conllup import readConlluFile \n",
    "sentences = readConlluFile(PATH_TREEBANK)\n",
    "\n",
    "if len(sentences) == 0:\n",
    "    raise ValueError(\"Le fichier est vide !\")\n",
    "else :\n",
    "    print(\"Le fichier contient {} phrases\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quoi ressemble notre objet \"sentence\"\n",
    "import json\n",
    "print(json.dumps(sentences[0], indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons retrouver, dans `sentence[\"metaJson\"]`, les meta-informations des phrases du conllu :\n",
    "- `sent_id`: identifiant de la phrase dans le treebank.\n",
    "- `sound_url`: lien url de l'audio de la phrase (ce treebank NaijaSynCor est un corpus oral) .\n",
    "- `speaker_id`: identifiant (anonymisé) du locuteur.\n",
    "- `text`: version \"prosodique\" du texte. Les ponctuations sont plus riches et des marqueurs prosodiques (retranscrivant l'oral) sont ajoutés.\n",
    "- `text_en`: traduction anglaise.\n",
    "- `text_ortho`: version littéraire du texte.\n",
    "- `timestamp`: date de la dernière annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voici donc ces meta-données seulement\n",
    "print(json.dumps(sentences[0]['metaJson'], indent=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment accéder aux tokens ?\n",
    "# ils sont imbriqués dans sentence['treeJson']['nodesJson']\n",
    "sentence_tokens = sentences[0]['treeJson']['nodesJson']\n",
    "print(json.dumps(sentence_tokens, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les tokens sont donc dans un dictionnaire avec comme clé l'ID du token \n",
    "# et comme valeur un dictionnaire avec les informations du token\n",
    "\n",
    "# Regardons le premier token\n",
    "print(json.dumps(sentence_tokens['1'], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici la composition d'un token au format json (voir [spécifications officielles ici](https://universaldependencies.org/format.html) )\n",
    "- `ID`: Identifiant du token dans la phrase,\n",
    "- `FORM`: Forme du token,\n",
    "- `LEMMA`: Lemme du token,\n",
    "- `UPOS`: Catégorie du discours universelle (Universal Part Of Speech)\n",
    "- `XPOS`: Catégorie du discours **optionnelle**\n",
    "- `FEATS`: Liste de features morphologiques de l'inventaire universel ou d'une extension spécifique à la langue\n",
    "- `HEAD`: Identifiant du gouverneur du token (0 si le token est la tête de la phrase)\n",
    "- `DEPREL`: Relation de dépendance avec le gouverneur (`root` si le token est la tête de la phrase)\n",
    "- `DEPS`: Graphes de dépendances améliorés sous la forme d'une liste de paires tête-déprel \n",
    "- `MISC`: Toutes les autres données non universelles peuvent être insérées dans cet objet MISC. Voici quelques exemples de valeur misc\n",
    "    - `AlignBegin`: alignement temporelle **du début** du token dans l'audio de la phrase (en milliseconde)\n",
    "    - `AlignEnd`: alignement temporelle de **la fin** du token dans l'audio de la phrase (en milliseconde),\n",
    "    - `Gloss`: glose anglaise\n",
    "    - `upos_pred`: Catégorie du dicours **prédit**\n",
    "    - `head_pred`: Identifiant **prédit** du gouverneur,\n",
    "    - `deprel_pred`: Relation de dépendance **prédite**,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant qu'on connait la structure, on peut compter le nombre de tokens dans tous le corpus\n",
    "nb_tokens = 0\n",
    "for sentence in sentences:\n",
    "    nb_tokens += len(sentence['treeJson']['nodesJson'])\n",
    "print(\"Le corpus contient {} tokens\".format(nb_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculons le nombre de tokens par phrase en moyenne dans ce corpus\n",
    "nb_tokens = 0\n",
    "for sentence in sentences:\n",
    "    nb_tokens += len(sentence['treeJson']['nodesJson'])\n",
    "print(\"Le corpus contient {} tokens\".format(nb_tokens))\n",
    "print(\"Le corpus contient {} phrases\".format(len(sentences)))\n",
    "print(\"Le nombre moyen de tokens par phrase est de {}\".format(nb_tokens/len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "Regardons nos données via des graphes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous utilisons matplotlib pour faire faire nos graphiques\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Voici des paramètres pour que les graphiques soient plus lisibles, changez les à votre convenance\n",
    "params = {\n",
    "   'axes.labelsize': 8,\n",
    " #  'text.fontsize': 8,\n",
    "   'legend.fontsize': 10,\n",
    "   'xtick.labelsize': 10,\n",
    "   'ytick.labelsize': 10,\n",
    "   'text.usetex': False,\n",
    "   'figure.figsize': [10, 5]\n",
    "   }\n",
    "mpl.rcParams.update(params)\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # pour avoir un fond gris quadrillé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichons la distribution des tokens par phrase\n",
    "\n",
    "## Recupérons le nombre de tokens par phrase dans une liste\n",
    "tokens_per_sentence = []\n",
    "for sentence in sentences:\n",
    "    tokens_per_sentence.append(len(sentence['treeJson']['nodesJson']))\n",
    "\n",
    "## Traçons l'histogramme\n",
    "plt.hist(tokens_per_sentence, bins=range(0, 60), edgecolor='white') \n",
    "plt.title(\"Distribution du nombre de tokens par phrase (ajusté)\") # ajoute un titre au graphique\n",
    "plt.xlabel(\"Nombre de tokens\") # ajoute un titre à l'axe des abscisses\n",
    "plt.ylabel(\"Nombre de phrases\") # ajoute un titre à l'axe des ordonnées\n",
    "\n",
    "\n",
    "## Optionel : Ajoutons une ligne verticale rouge pour la moyenne\n",
    "length_mean = nb_tokens/len(sentences)\n",
    "plt.axvline(x=length_mean, color='red', linestyle='dashed', linewidth=1) # place la ligne verticale de moyenne (en rouge)\n",
    "plt.text(length_mean*1.1, plt.ylim()[1]*0.9, 'Moyenne = {:.2f}'.format(length_mean)) # place la légende de la ligne verticale\n",
    "\n",
    "\n",
    "## Affichons le graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faisons une fonction \"counter\" qui, depuis une liste d'étiquettes, retourne un dictionnaire avec le nombre d'occurences de chaque étiquette\n",
    "# le compteur doit être ordonné par ordre alphabétique\n",
    "def make_counter(labels):\n",
    "    counter = {}\n",
    "    for label in labels:\n",
    "        if label in counter:\n",
    "            counter[label] += 1\n",
    "        else:\n",
    "            counter[label] = 1\n",
    "    # sort alphabetically before return\n",
    "    counter = {k: v for k, v in sorted(counter.items(), key=lambda item: item[0])}\n",
    "    return counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# testons sur une liste exemple\n",
    "labels = [\"VERB\", \"NOUN\", \"NOUN\", \"VERB\", \"VERB\"]\n",
    "print(make_counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichons la distribution des différentes catégories du discours (UPOS)\n",
    "\n",
    "## Recupérons le nombre de tokens par phrase dans une liste\n",
    "upos = []\n",
    "for sentence in sentences:\n",
    "    for token in sentence['treeJson']['nodesJson'].values():\n",
    "        upos.append(token['UPOS'])\n",
    "\n",
    "upos_counter = make_counter(upos)\n",
    "print(json.dumps(upos_counter, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traçons l'histogramme\n",
    "\n",
    "plt.bar(upos_counter.keys(), upos_counter.values())\n",
    "plt.xticks(rotation=45) # pour que les étiquettes soient lisibles (on les tourne de 45°)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les ponctuations sont grandement dominantes, il faudra faire attention à ce que ça ne biaise pas les évaluations du parseur (il est généralement plus facile de faire des prédictions sur les ponctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance et ordre syntaxique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le naija est t'il une langue à tête initiale ou finale ?\n",
    "# Pour cela, nous allons compter le nombre de dépendances allant vers la gauche et vers la droite\n",
    "left_n = 0\n",
    "right_n = 0\n",
    "for sentence in sentences:\n",
    "    for token in sentence['treeJson']['nodesJson'].values():\n",
    "        if token['ID'] == '0':\n",
    "            # On ne compte pas les racines\n",
    "            continue\n",
    "\n",
    "        if token['HEAD'] < int(token['ID']):\n",
    "            right_n += 1\n",
    "        else:\n",
    "            left_n += 1\n",
    "print(\"Il y a {} dépendances vers la gauche et {} vers la droite\".format(left_n, right_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Est-ce que ces directions sont différentes selon les catégories du discours ?\n",
    "# Pour cela, nous allons compter le nombre de dépendances allant vers la gauche et vers la droite pour chaque catégorie du discours\n",
    "# Nous allons utiliser un dictionnaire de dictionnaires pour stocker ces informations\n",
    "# Le premier niveau de clé sera la catégorie du discours\n",
    "# Le second niveau de clé sera la direction de la dépendance (left ou right)\n",
    "# La valeur sera le nombre de dépendances dans cette catégorie et dans cette direction\n",
    "\n",
    "upos_directions = {}\n",
    "for sentence in sentences:\n",
    "    for token in sentence['treeJson']['nodesJson'].values():\n",
    "        if token['ID'] == '0':\n",
    "            # On ne compte pas les racines\n",
    "            continue\n",
    "\n",
    "        if token['UPOS'] not in upos_directions:\n",
    "            upos_directions[token['UPOS']] = {'left': 0, 'right': 0}\n",
    "\n",
    "        if token['HEAD'] < int(token['ID']):\n",
    "            upos_directions[token['UPOS']]['right'] += 1\n",
    "        else:\n",
    "            upos_directions[token['UPOS']]['left'] += 1\n",
    "upos_directions = {k: v for k, v in sorted(upos_directions.items(), key=lambda item: item[0])}        \n",
    "print(json.dumps(upos_directions, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons maintenant afficher ces informations dans un graphique en barres\n",
    "# Deux options s'offrent à nous :\n",
    "# - un graphique en barres empilées\n",
    "# - un graphique en barres côte à côte\n",
    "# Nous allons utiliser un graphique en barres côte à côte\n",
    "\n",
    "import numpy as np\n",
    "# Séparons les données dans des listes \"droite\" et \"gauche\"\n",
    "labels = list(upos_directions.keys())\n",
    "left_values = [upos['left'] for upos in upos_directions.values()]\n",
    "right_values = [upos['right'] for upos in upos_directions.values()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35  # épaisseur des barres\n",
    "index = np.arange(len(labels))  # position des barres\n",
    "\n",
    "# Traçage des barres de gauche\n",
    "left_bars = ax.bar(index, left_values, bar_width, label='Gauche', color='skyblue')\n",
    "\n",
    "# Traçage des barres de gauche\n",
    "right_bars = ax.bar(index + bar_width, right_values, bar_width, label='Droite', color='orange')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('étiquettes UPOS')\n",
    "ax.set_ylabel('Comptes')\n",
    "ax.set_title('Distribution des directions des dépendances par étiquette UPOS : Gauche vs Droite')\n",
    "ax.set_xticks(index + bar_width / 2)  # Position x-axis ticks in the center of the two bars\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Rotate the x-axis labels to 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout to fit everything\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce graphique est intéressant, mais on voudrait aussi connaitre la direction en fonction de la \n",
    "# catégorie du discours du gouverneur (HEAD) et afficher cela dans une matrice de confusion\n",
    "\n",
    "# Pour cela, nous allons d'abord faire notre dictionnaire de dictionnaire de dictionnaire\n",
    "# Le premier niveau de clé sera la catégorie du discours du gouverneur (HEAD)\n",
    "# Le second niveau de clé sera la catégorie du discours du dépendant (DEP)\n",
    "# Le troisième niveau de clé sera la direction de la dépendance (left ou right)\n",
    "# La valeur sera le nombre de dépendances dans cette catégorie et dans cette direction\n",
    "\n",
    "head_dep_directions = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    for token in sentence['treeJson']['nodesJson'].values():\n",
    "        if token['HEAD'] == 0:\n",
    "            # On ne compte pas les cas ou la tête est la racine\n",
    "            continue\n",
    "        head_token = sentence['treeJson']['nodesJson'][str(token['HEAD'])]\n",
    "        head_ID = head_token['ID']\n",
    "        head_UPOS = head_token['UPOS']\n",
    "\n",
    "        this_upos = token['UPOS']\n",
    "        if head_UPOS not in head_dep_directions:\n",
    "            head_dep_directions[head_UPOS] = {}\n",
    "\n",
    "        if this_upos not in head_dep_directions[head_UPOS]:\n",
    "            head_dep_directions[head_UPOS][this_upos] = {'left': 0, 'right': 0}\n",
    "\n",
    "        if token['HEAD'] < int(token['ID']):\n",
    "            head_dep_directions[head_UPOS][this_upos]['right'] += 1\n",
    "        else:\n",
    "            head_dep_directions[head_UPOS][this_upos]['left'] += 1\n",
    "\n",
    "# On trie le dictionnaire par ordre alphabétique\n",
    "head_dep_directions = {k: v for k, v in sorted(head_dep_directions.items(), key=lambda item: item[0])}\n",
    "# Et on trie les dictionnaires imbriqués par ordre alphabétique\n",
    "for head_upos in head_dep_directions:\n",
    "    head_dep_directions[head_upos] = {k: v for k, v in sorted(head_dep_directions[head_upos].items(), key=lambda item: item[0])}\n",
    "\n",
    "\n",
    "print(json.dumps(head_dep_directions, indent=4))\n",
    "\n",
    "# Nous allons maintenant afficher ces informations dans une matrice de confusion\n",
    "# Pour cela, nous allons utiliser la librairie matplotlib\n",
    "\n",
    "# Nous allons utiliser une fonction de la librairie matplotlib pour afficher la matrice de confusion\n",
    "# Cette fonction prend en paramètre une matrice de confusion et une liste d'étiquettes\n",
    "# Elle affiche la matrice de confusion dans un graphique\n",
    "# Elle retourne un objet \"figure\" qui contient le graphique\n",
    "# Nous allons utiliser cet objet pour ajouter un titre à notre graphique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour la matricde de confusion, il nous faut, pour chaque paire de catégories du discours (HEAD, DEP),\n",
    "# un ratio de gouvernance à droite par rapport au nombre total de gouvernance\n",
    "# Nous allons donc créer un dictionnaire de dictionnaire de dictionnaire de ratio\n",
    "# Le premier niveau de clé sera la catégorie du discours du gouverneur (HEAD)\n",
    "# Le second niveau de clé sera la catégorie du discours du dépendant (DEP)\n",
    "\n",
    "head_dep_ratios = {}\n",
    "head_dep_total = {}\n",
    "\n",
    "for head_upos in head_dep_directions:\n",
    "    head_dep_ratios[head_upos] = {}\n",
    "    head_dep_total[head_upos] = {}\n",
    "    for dep_upos in head_dep_directions[head_upos]:\n",
    "        left_relation = head_dep_directions[head_upos][dep_upos]['left']\n",
    "        right_relation = head_dep_directions[head_upos][dep_upos]['right']\n",
    "        total_relation = left_relation + right_relation\n",
    "        if total_relation < 10:\n",
    "            head_dep_ratios[head_upos][dep_upos] = 0\n",
    "        else:\n",
    "            head_dep_ratios[head_upos][dep_upos] = (right_relation + 0.0001) / total_relation \n",
    "        head_dep_total[head_upos][dep_upos] = total_relation\n",
    "\n",
    "print(json.dumps(head_dep_ratios, indent=4))\n",
    "print(json.dumps(head_dep_total, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dep_ratios['PUNCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons d'abord afficher le nombre de relation par pair de catégories du discours\n",
    "# Il est important de faire cela avant d'afficher le ratio, car le ratio ne veut rien dire si le nombre de relation est trop faible\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Créer un DataFrame pandas pour stocker les données\n",
    "upos_df = pd.DataFrame(head_dep_total)\n",
    "confusion_matrix = upos_df.groupby(upos_df.columns, axis=1).sum().astype(int)\n",
    "\n",
    "# Créer la matrice de confusion avec seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d')\n",
    "\n",
    "# Ajouter les titres et labels\n",
    "plt.title('Nombre de dépendances par catégorie du discours du gouverneur (HEAD) et du dépendant (DEP)')\n",
    "plt.xlabel('Gouverneurs')\n",
    "plt.ylabel('Dépendants')\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Créer un DataFrame pandas pour stocker les données\n",
    "upos_df = pd.DataFrame(head_dep_ratios)\n",
    "# Pour les besoins de la visualisation, nous allons sommer les valeurs de 'left' et 'right'\n",
    "# pour chaque paire d'étiquettes UPOS\n",
    "confusion_matrix = upos_df.groupby(upos_df.columns, axis=1).sum()\n",
    "confusion_matrix.replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Créer la matrice de confusion avec seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='.2f')\n",
    "\n",
    "# Ajouter les titres et labels\n",
    "plt.title('Ratio de dépendances à droite par rapport au nombre total de dépendances')\n",
    "plt.xlabel('Gouverneurs')\n",
    "plt.ylabel('Dépendants')\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions :\n",
    "Dans ce treebank du Naija SUD :\n",
    "- L'auxiliaire est-t'il plus souvent dépendant ou gouverneur ? \n",
    "- Une ponctuation peut-elle gouverner une non-ponctuation ? Arriverez vous à retrouver ces cas dans le treebank ?\n",
    "- Quelles sont les catégories qui gouvernent le plus ?\n",
    "- Et quelles sont les catégories qui gouvernent le moins ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques algos supplémentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculons le nombre de nom propre/nom commun par phrase en moyenne dans ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculons le nombre de verbe par phrase en moyenne dans ce corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quels sont les trigrammes syntaxiques les plus fréquents dans ce corpus ?\n",
    "# On définit un trigramme comme un ensemble de trois tokens consécutifs, dont deux sont dépendants du troisième\n",
    "\n",
    "trigrams = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = list(sentence['treeJson']['nodesJson'].values())\n",
    "    for i in range(len(tokens) - 2):\n",
    "        token1 = tokens[i]\n",
    "        token2 = tokens[i+1]\n",
    "        token3 = tokens[i+2]\n",
    "\n",
    "        # on ne compte pas si l'un des tokens est une ponctuation\n",
    "        if token1['UPOS'] == 'PUNCT' or token2['UPOS'] == 'PUNCT' or token3['UPOS'] == 'PUNCT':\n",
    "            continue\n",
    "        # if token1['HEAD'] == token2['HEAD'] == token3['HEAD']:\n",
    "        #     # On ne compte pas les trigrammes dont les trois tokens ont le même gouverneur\n",
    "        #     continue\n",
    "        trigram_form = \" , \".join([token1['FORM'], token2['FORM'], token3['FORM']])\n",
    "        if trigram_form not in trigrams:\n",
    "            trigrams[trigram_form] = 0\n",
    "        trigrams[trigram_form] += 1\n",
    "\n",
    "# sort by decreasing frequency\n",
    "trigrams = {k: v for k, v in sorted(trigrams.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(json.dumps(trigrams, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce n'est pas pertinent, on a juste les trigrammes les plus fréquents, mais pas\n",
    "# les plus dépendants les uns des autres\n",
    "# Il faut donc diviser par la multiplication des fréquences de chaque token\n",
    "\n",
    "# On récupère donc les fréquences des tokens\n",
    "monogrammes = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    for token in sentence['treeJson']['nodesJson'].values():\n",
    "        if token['UPOS'] == 'PUNCT':\n",
    "            continue\n",
    "        if token['FORM'] not in monogrammes:\n",
    "            monogrammes[token['FORM']] = 0\n",
    "        monogrammes[token['FORM']] += 1\n",
    "\n",
    "trigrams_PMI = {}\n",
    "for trigram in trigrams:\n",
    "    token1, token2, token3 = trigram.split(\" , \")\n",
    "    trigrams_PMI[trigram] = trigrams[trigram] / (monogrammes[token1] * monogrammes[token2] * monogrammes[token3])\n",
    "\n",
    "# sort by decreasing frequency\n",
    "trigrams_PMI = {k: v for k, v in sorted(trigrams.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "\n",
    "print(json.dumps(trigrams_PMI, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce n'est encore pas si interessant, puisque nous avons en tête des trigrammes qui n'apparaissent qu'une seule fois\n",
    "# Nous allons donc filtrer les trigrammes qui apparaisent moins de N fois (par exemple 10 fois)\n",
    "\n",
    "trigrams_PMI = {}\n",
    "\n",
    "\n",
    "N = 5 # changez cette valeur pour voir les trigrammes qui apparaissent plus ou moins souvent\n",
    "# au plus cette valeur sera basse, au plus on aura de trigrammes irréguliers (des \"outliers\" non représentatifs)\n",
    "# par exemple, en dessous de N=5, vous verrez beaucoup de noms propres et suite de numéraux\n",
    "\n",
    "for trigram in list(trigrams.keys()):\n",
    "    \n",
    "    if trigrams[trigram] > N:\n",
    "        token1, token2, token3 = trigram.split(\" , \")\n",
    "        trigrams_PMI[trigram] = trigrams[trigram] / (monogrammes[token1] * monogrammes[token2] * monogrammes[token3])\n",
    "\n",
    "trigrams_PMI = {k: v for k, v in sorted(trigrams_PMI.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(json.dumps(trigrams_PMI, indent=4))\n",
    "print(\"{} trigrammes ont été trouvés\".format(len(trigrams_PMI)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "### D'autres concepts interessants\n",
    "- L'espagnol est une langue pro-drop, c'est à dire que le sujet pronominal d'un verbe peut être effacé. Qu'en est t-il du naija ? Quel est le ratio de verbe sans sujet ? Est-ce que ce ratio seul suffit à proposer une hypothèse ?\n",
    "- Nous n'avons pas encore traçé les distributions des longueurs de dépendances syntaxique.\n",
    "\n",
    "### Comparaison \n",
    "Vous pouvez trouver dans le dossier `data` d'autres treebanks SUD. On peut donc comparer certains des indicateurs que nous avons calculés ici avec ceux des autres langues.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
