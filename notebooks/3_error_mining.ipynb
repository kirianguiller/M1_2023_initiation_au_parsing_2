{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce notebook est de trouver les erreurs les plus fréquentes du parseur afin de corriger le treebank en ayant le plus d'impact possible. Nous regarderons donc quelles sont les types d'erreurs faites par le parseur, quelles sont les plus problématiques et quelle distribution ont ces erreurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TREEBANK = \"../data/sud_naija-NSC.with_prediction.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le fichier est t-il bien présent ?\n",
    "try:\n",
    "    with open(PATH_TREEBANK, \"r\") as f:\n",
    "        print(\"Le fichier est présent au chemin : {}\".format(PATH_TREEBANK))\n",
    "        pass\n",
    "except FileNotFoundError:\n",
    "    print(\"Le fichier n'est PAS , revoyez le chemin : {}\".format(PATH_TREEBANK))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut commencer à parser le fichier avec la librairie conllup\n",
    "from conllup.conllup import readConlluFile \n",
    "sentences = readConlluFile(PATH_TREEBANK)\n",
    "\n",
    "if len(sentences) == 0:\n",
    "    raise ValueError(\"Le fichier est vide !\")\n",
    "else :\n",
    "    print(\"Le fichier contient {} phrases\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence de la taille de la phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les nombres de réussite et d'échec pour chaque longueur de phrase\n",
    "results_per_length = {}\n",
    "def get_default_result():\n",
    "    return {\n",
    "            \"total_token\": 0,\n",
    "            \"success_upos\": 0, \n",
    "            \"success_deprel\": 0, \n",
    "            \"success_head\": 0, \n",
    "            \"success_head_and_deprel\": 0, \n",
    "        }\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = sentence[\"treeJson\"][\"nodesJson\"].values()\n",
    "    length = len(tokens)\n",
    "    if length not in results_per_length:\n",
    "        results_per_length[length] = get_default_result()\n",
    "    \n",
    "    for token in tokens:\n",
    "        head_token_ID = str(token['HEAD'])\n",
    "        deprel = token['DEPREL']\n",
    "        upos = token['UPOS']\n",
    "\n",
    "        head_pred = token['MISC']['head_pred']\n",
    "        deprel_pred = token['MISC']['deprel_pred']\n",
    "        upos_pred = token['MISC']['upos_pred']\n",
    "\n",
    "        if head_token_ID == head_pred:\n",
    "            results_per_length[length][\"success_head\"] += 1\n",
    "\n",
    "        if deprel == deprel_pred:\n",
    "            results_per_length[length][\"success_deprel\"] += 1\n",
    "        \n",
    "        if head_token_ID == head_pred and deprel == deprel_pred:\n",
    "            results_per_length[length][\"success_head_and_deprel\"] += 1\n",
    "\n",
    "        if upos == upos_pred:\n",
    "            results_per_length[length][\"success_upos\"] += 1\n",
    "\n",
    "        results_per_length[length][\"total_token\"] += 1\n",
    "\n",
    "# On affiche les résultats\n",
    "import json\n",
    "print(json.dumps(results_per_length, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous utilisons matplotlib pour faire faire nos graphiques\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Voici des paramètres pour que les graphiques soient plus lisibles, changez les à votre convenance\n",
    "params = {\n",
    "   'axes.labelsize': 8,\n",
    " #  'text.fontsize': 8,\n",
    "   'legend.fontsize': 10,\n",
    "   'xtick.labelsize': 10,\n",
    "   'ytick.labelsize': 10,\n",
    "   'text.usetex': False,\n",
    "   'figure.figsize': [10, 5]\n",
    "   }\n",
    "mpl.rcParams.update(params)\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # pour avoir un fond gris quadrillé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essayons de mettre tout ça dans un graphique\n",
    "# On va plotter les ratio d'echec (1 - ratio_reussite) ça sera plus visuel\n",
    "# Dans un même graphique on va plotter les 3 ratios : upos, deprel, head\n",
    "\n",
    "colors = [\"red\", \"green\", \"blue\", \"orange\"]\n",
    "\n",
    "x_lim = [5, 80] # on ne prend pas en compte les phrases de moins de 5 tokens et de plus de 80 tokens\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i, label in enumerate([\"deprel\", \"upos\", \"head\"]):\n",
    "    x = list(range(x_lim[0], x_lim[1]))\n",
    "    y = []\n",
    "    for length in x:\n",
    "        number_success = results_per_length.get(length, get_default_result())[\"success_{}\".format(label)]\n",
    "        number_total = results_per_length.get(length, get_default_result())[\"total_token\"]\n",
    "        \n",
    "        fail_ratio = 0\n",
    "        if number_total != 0:\n",
    "            fail_ratio = (number_total - number_success) / number_total\n",
    "        \n",
    "        y.append(fail_ratio)\n",
    "\n",
    "    plt.plot(range(x_lim[0], x_lim[1]), y, label=label, color=colors[i])\n",
    "\n",
    "    # # si on veut afficher les approximations linéaires :\n",
    "    # from sklearn.linear_model import LinearRegression\n",
    "    # import numpy as np\n",
    "    # regressor = LinearRegression()  \n",
    "    # x = np.array(x).reshape(-1, 1)\n",
    "    # y = np.array(y).reshape(-1, 1)\n",
    "    # regressor.fit(x, y)\n",
    "    # plt.plot(x, regressor.predict(x) ,color=colors[i], linestyle=\"dotted\")\n",
    "\n",
    "plt.xlabel(\"taille de la phrase\")\n",
    "plt.ylabel(\"probabilité d'erreur pour un token\")\n",
    "plt.legend()\n",
    "plt.xlim([x_lim[0]+1,x_lim[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Est-ce que ces résultats étaient attendus ?\n",
    "- À quel(s) biais devons nous faire attention ?\n",
    "\n",
    "NB : Il faudrait utiliser des tests de significativité afin de prouver que les corrélations linéaires ne soient pas dû au hasard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erreurs de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère d'abord les listes des features réelles et prédites\n",
    "gold_uposs = []\n",
    "gold_deprels = []\n",
    "gold_heads = []\n",
    "\n",
    "pred_uposs = []\n",
    "pred_deprels = []\n",
    "pred_heads = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = sentence[\"treeJson\"][\"nodesJson\"].values()\n",
    "    for token in tokens:\n",
    "        gold_uposs.append(token['UPOS'])\n",
    "        gold_deprels.append(token['DEPREL'])\n",
    "        gold_heads.append(token['HEAD'])\n",
    "\n",
    "        pred_uposs.append(token['MISC']['upos_pred'])\n",
    "        pred_deprels.append(token['MISC']['deprel_pred'])\n",
    "        pred_heads.append(token['MISC']['head_pred'])\n",
    "\n",
    "set_uposs = set(gold_uposs + pred_uposs)\n",
    "set_deprels = set(gold_deprels + pred_deprels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erreurs de POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelles sont les plus grosses confusions de POS ?\n",
    "\n",
    "# On va créer un dictionnaire qui va contenir pour chaque couple de POS, le nombre de fois où on a eu cette confusion\n",
    "confusion_matrix_upos = {}\n",
    "sorted_set_uposs = sorted(set_uposs)\n",
    "\n",
    "for gold_upos in sorted_set_uposs:\n",
    "    confusion_matrix_upos[gold_upos] = {}\n",
    "    for pred_upos in sorted_set_uposs:\n",
    "        confusion_matrix_upos[gold_upos][pred_upos] = 0\n",
    "\n",
    "for pred_upos, gold_upos in zip(pred_uposs, gold_uposs):\n",
    "    confusion_matrix_upos[gold_upos][pred_upos] += 1\n",
    "\n",
    "# On affiche le dictionnaire\n",
    "print(json.dumps(confusion_matrix_upos, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons d'abord afficher le nombre de relation par pair de catégories du discours\n",
    "# Il est important de faire cela avant d'afficher le ratio, car le ratio ne veut rien dire si le nombre de relation est trop faible\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Créer un DataFrame pandas pour stocker les données\n",
    "upos_df = pd.DataFrame(confusion_matrix_upos)\n",
    "confusion_matrix = upos_df.groupby(upos_df.columns, axis=1).sum().astype(int)\n",
    "\n",
    "# Créer la matrice de confusion avec seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d')\n",
    "\n",
    "# Ajouter les titres et labels\n",
    "plt.title('Matrice de confusion (UPOS)')\n",
    "plt.xlabel('Prédits')\n",
    "plt.ylabel('Réels')\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combien d'adjectifs ont été annotés comme des noms ?\n",
    "- Quelle est la catégorie la plus annotée ?\n",
    "- Quelle est la catégorie la mieux prédite ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erreurs des deprels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelles sont les plus grosses confusions de deprel ?\n",
    "\n",
    "# On va créer un dictionnaire qui va contenir pour chaque couple de deprel, le nombre de fois où on a eu cette confusion\n",
    "confusion_matrix_deprel = {}\n",
    "sorted_set_deprels = sorted(set_deprels)\n",
    "\n",
    "for gold_deprel in sorted_set_deprels:\n",
    "    confusion_matrix_deprel[gold_deprel] = {}\n",
    "    for pred_deprel in sorted_set_deprels:\n",
    "        confusion_matrix_deprel[gold_deprel][pred_deprel] = 0\n",
    "\n",
    "for pred_deprel, gold_deprel in zip(pred_deprels, gold_deprels):\n",
    "    confusion_matrix_deprel[gold_deprel][pred_deprel] += 1\n",
    "\n",
    "# On affiche le dictionnaire\n",
    "print(json.dumps(confusion_matrix_deprel, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons d'abord afficher le nombre de relation par pair de catégories du discours\n",
    "# Il est important de faire cela avant d'afficher le ratio, car le ratio ne veut rien dire si le nombre de relation est trop faible\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Créer un DataFrame pandas pour stocker les données\n",
    "deprel_df = pd.DataFrame(confusion_matrix_deprel)\n",
    "\n",
    "\n",
    "# # Gardons uniquement les relations qui contiennent \"comp\"\n",
    "# deprels_to_keep = [deprel for deprel in sorted(set_deprels) if \"mod\" in deprel]\n",
    "# deprel_df = deprel_df.loc[deprels_to_keep, deprels_to_keep]\n",
    "\n",
    "confusion_matrix = deprel_df.groupby(deprel_df.columns, axis=1).sum().astype(int)\n",
    "\n",
    "# Créer la matrice de confusion avec seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d')\n",
    "\n",
    "# Ajouter les titres et labels\n",
    "plt.title('Matrice de confusion (deprel)')\n",
    "plt.xlabel('Prédits')\n",
    "plt.ylabel('Réels')\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
